# æŠŠ core.py é¡¶éƒ¨ç°æœ‰çš„ argparse.parse_args() æ®µæ›¿æ¢ä¸ºä¸‹é¢è¿™æ®µï¼š

import argparse
import sys

# å½“ core è¢«å…¶ä»–æ¨¡å— importï¼ˆæ¯”å¦‚ graph_experiment.pyï¼‰æ—¶ï¼Œ
# ä¸è¦ç”¨ parse_args() å»æ¶ˆè´¹å¤–éƒ¨ä¼ å…¥çš„æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œ
# å¦åˆ™ä¼šå¯¼è‡´ import æ—¶ argparse æŠ¥é”™ï¼ˆunrecognized argumentsï¼‰ã€‚
# ä½¿ç”¨ parse_known_args()ï¼šåªè§£ææˆ‘ä»¬å…³å¿ƒçš„ --taskï¼Œå¿½ç•¥æœªçŸ¥å‚æ•°ã€‚
from utils import load_config

DATA_CONFIG, EXPERIMENT_CONFIG = load_config()

import time
import random
import numpy as np
import re
from sklearn.cluster import KMeans
from typing import List, Dict, Any, Optional
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio

from llm import call_openai
from mapgd_tasks import get_mapgd_task_class
from mapgd_predictors import get_mapgd_predictor
from utils import MAPGDUtils
from abc import abstractmethod, ABCMeta
from hcgc_caaw import HypersphereConstrainedGradientClustering

# Optional GPU support - graceful fallback if torch unavailable
try:
    import torch

    GPU_AVAILABLE = torch.cuda.is_available()
except ImportError:
    torch = None
    GPU_AVAILABLE = False


class MAPGDFramework(metaclass=ABCMeta):
    """
    MAPGD: Multi-Agent Prompt Gradient Descent Framework

    Implements collaborative prompt optimization through:
    1. Multi-agent exploration with specialized roles
    2. Semantic gradient coordination and fusion
    3. Beam search + Monte Carlo prompt expansion
    4. Bandit-based efficient candidate selection

    Based on the paper: "MAPGD: Multi-Agent Prompt Gradient Descent
    for Collaborative Prompt Optimization"
    """

    def __init__(self, config):
        # Core MAPGD components initialization
        self.config = config
        self.max_iterations = config.get('max_iterations', 10)

        # Multi-agent collaboration module
        self.agent_manager = AgentManager(config)

        # Gradient coordination module (semantic clustering + conflict resolution)
        self.gradient_coordinator = GradientCoordinator(config)

        # Collaborative prompt expansion (beam search + MC paraphrasing)
        self.prompt_expander = CollaborativePromptExpander(config)

        # Bandit-based candidate selection
        self.candidate_selector = BanditBasedSelector(config)

        # Convergence monitoring
        self.convergence_monitor = ConvergenceMonitor(config)

        print(f"MAPGD Framework initialized with {config.get('num_agents', 4)} specialized agents")

    @abstractmethod
    def optimize_prompt(self, initial_prompt, task_name, data_dir, predictor=None):
        """
        Main MAPGD optimization workflow

        Implements the collaborative gradient descent process:
        1. Multi-agent gradient generation (parallel exploration)
        2. Semantic gradient coordination and fusion
        3. Collaborative prompt expansion via beam search
        4. Bandit-based efficient candidate selection
        5. Agent synchronization and convergence monitoring

        Returns optimized prompt with performance history
        """
        pass


class SpecializedPromptAgent:
    """
    Specialized Prompt Agent for MAPGD Framework

    Each agent focuses on a specific aspect of prompt optimization:
    - instruction_specialist: Task description and instruction clarity
    - example_curator: Few-shot example selection and formatting
    - format_designer: Output format and structure design
    - style_optimizer: Language style and tone adjustment

    Implements role-specific gradient generation and prompt updating
    """

    #     AGENT_TYPES = {
    #     "instruction_specialist": {
    #         "focus": "Clarity and completeness of task instructions for report generation and data verification",
    #         "expertise": "Analyze and improve instructions to ensure clarity, completeness, and enforceability, with specific guidance for verifying input data authenticity and generating structured reports",
    #         "gradient_type": "instruction_gradient",
    #         "role_description": "Specializes in crafting clear, complete, and executable instructions for generating truthful, structured reports and rejecting false or exaggerated inputs. Instructions must include steps to verify input data (e.g., checking financial metrics, timelines, or scales for realism) and rules for producing professional business, financial, and legal reports.",
    #         "prompt": "You are an instruction specialist tasked with improving the clarity and completeness of task instructions for a model that generates truthful, structured reports and identifies false or exaggerated inputs. Your goal is to ensure the instructions are clear, enforceable, and include: 1) A step to verify input data authenticity by checking for realistic metrics (e.g., ARR in millions, not negative or absurd values), timelines (e.g., plausible years like 2025, not 11975), and scales (e.g., reasonable team sizes). 2) Rules for generating structured reports with Business Analysis, Financial Report, and Legal Prompt sections, ensuring professional and verifiable content. 3) Guidance to reject invalid inputs with a clear error message. Revise the instructions to be concise, unambiguous, and task-specific, incorporating examples like: Valid input ('$4.16M ARR') should produce a structured report; invalid input ('$-46,110,7-458 ARR') should return '<Error> Invalid input: Exaggerated or unverifiable claims detected.'"
    #     },
    #     "example_curator": {
    #         "focus": "Selection and formatting of representative examples for report generation and false data detection",
    #         "expertise": "Optimize the selection of diverse, representative examples from positive (truthful) and negative (false/exaggerated) samples, ensuring consistent formatting and clear distinction between valid and invalid inputs",
    #         "gradient_type": "example_gradient",
    #         "role_description": "Focuses on selecting and formatting diverse examples that demonstrate truthful report generation and false data rejection. Examples must include positive samples (e.g., realistic ARR, market size) and negative samples (e.g., exaggerated financials, absurd timelines), formatted consistently to guide the model in distinguishing valid from invalid inputs.",
    #         "prompt": "You are an example curator tasked with selecting and formatting representative examples to guide a model in generating truthful, structured reports and rejecting false or exaggerated inputs. Your goal is to: 1) Select diverse examples from positive samples (e.g., '$4.16M ARR', 'construction market valued at $50B') and negative samples (e.g., '$-46,110,7-458 ARR', '1050 FTEs by 11975'). 2) Ensure examples highlight key differences, such as realistic vs. absurd financial metrics, timelines, or scales. 3) Format examples consistently with fields: Section, Statement, Context, and Output (structured report for valid inputs, error message for invalid inputs). For example: Positive sample: 'Section: Key Insights, Statement: The company has demonstrated strong revenue growth with $4.16M in ARR, Context: SaaS startup report, Output: <Business Analysis>..., <Financial Report>..., <Legal Prompt>...'; Negative sample: 'Section: ARR, Statement: $-46,110,7-458, Context: ARR section, Output: <Error> Invalid input: Exaggerated or unverifiable claims detected.' Optimize the example set for diversity, representativeness, and clarity."
    #     },
    #     "format_designer": {
    #         "focus": "Design of structured output formats for reports and error messages",
    #         "expertise": "Design clear, consistent output templates for structured reports (Business Analysis, Financial Report, Legal Prompt) and error messages for invalid inputs",
    #         "gradient_type": "format_gradient",
    #         "role_description": "Specializes in designing clear, structured output templates for truthful report generation and false data rejection. Ensures outputs use consistent tags (<Business Analysis>, <Financial Report>, <Legal Prompt> for valid inputs; <Error> for invalid inputs) and are easy for the model to parse and generate.",
    #         "prompt": "You are a format designer tasked with creating clear, structured output templates for a model that generates truthful reports and rejects false or exaggerated inputs. Your goal is to: 1) Design a consistent output format for valid inputs, using tags: <Business Analysis>, <Financial Report>, <Legal Prompt>, with clear section boundaries and professional content. 2) Design an error format for invalid inputs: <Error> Invalid input: Exaggerated or unverifiable claims detected. 3) Ensure formats are parseable and unambiguous. For example: Valid input ('$4.16M ARR') should output: <Business Analysis> [Market analysis and recommendations] </Business Analysis> <Financial Report> [Financial impacts and risks] </Financial Report> <Legal Prompt> [Legal compliance analysis] </Legal Prompt>; Invalid input ('$-46,110,7-458 ARR') should output: <Error> Invalid input: Exaggerated or unverifiable claims detected. Optimize the format for clarity, consistency, and task alignment."
    #     },
    #     "style_optimizer": {
    #         "focus": "Professional language style and tone for report generation and error messages",
    #         "expertise": "Optimize language for professionalism, conciseness, and adaptability to business and legal contexts, ensuring error messages are clear and direct",
    #         "gradient_type": "style_gradient",
    #         "role_description": "Optimizes language style and tone for generating professional, truthful reports and clear error messages. Ensures reports use concise, industry-appropriate language for business, financial, and legal contexts, and error messages are direct and unambiguous.",
    #         "prompt": "You are a style optimizer tasked with refining the language style and tone for a model that generates truthful, structured reports and rejects false or exaggerated inputs. Your goal is to: 1) Ensure report sections (<Business Analysis>, <Financial Report>, <Legal Prompt>) use professional, concise, and industry-appropriate language for business and legal contexts (e.g., 'The $4.16M ARR reflects strong market traction' instead of vague or informal phrasing). 2) Ensure error messages for invalid inputs (e.g., '$-46,110,7-458 ARR') are clear, direct, and professional (e.g., '<Error> Invalid input: Exaggerated or unverifiable claims detected'). 3) Avoid jargon overload or overly complex language. Optimize the style for professionalism, clarity, and task-specific adaptation, ensuring alignment with examples like: Positive output: 'The companyâ€™s $4.16M ARR indicates robust growth'; Negative output: '<Error> Invalid input: Exaggerated or unverifiable claims detected.'"
    #     }
    # }
    BASE_AGENT_TYPES = {
        'instruction_specialist': {
            'focus': 'Clarity of task descriptions and instructions',
            'expertise': 'Analyze the completeness, clarity, and enforceability of instructions',
            'gradient_type': 'instruction_gradient',
            'role_description': 'Specializes in analyzing and improving task instructions, ensuring clarity, completeness, and executability'
        },
        'example_curator': {
            'focus': 'Few-shot example selection and formatting',
            'expertise': 'Optimize the representativeness, diversity, and format consistency of examples',
            'gradient_type': 'example_gradient',
            'role_description': 'Focuses on selecting representative, diverse examples and ensuring consistent formatting'
        },
        'format_designer': {
            'focus': 'Output format and structure design',
            'expertise': 'Design clear output templates and structured formats',
            'gradient_type': 'format_gradient',
            'role_description': 'Designs clear output templates and structured formats for better model understanding'
        },
        'style_optimizer': {
            'focus': 'Language style and tone adjustments',
            'expertise': 'Optimize the professionalism and adaptability of language expression',
            'gradient_type': 'style_gradient',
            'role_description': 'Optimizes language expression for professionalism and task-specific adaptation'
        },
        'generic': {
            'focus': 'General prompt improvement',
            'expertise': 'Provide general reasons and improvements without specialization',
            'gradient_type': 'generic_gradient',
            'role_description': 'General agent without specialization, explores improvements broadly'
        }
    }

    # æ•°å­¦æ¨ç†ä¸“ç”¨æ™ºèƒ½ä½“ç±»å‹ (é€‚ç”¨äºGSM8kç­‰æ•°å­¦ä»»åŠ¡)
    MATH_AGENT_TYPES = {
        'reasoning_specialist': {
            'focus': 'Mathematical reasoning process and step-by-step logic',
            'expertise': 'Analyze and improve the logical flow, step clarity, and reasoning structure in mathematical problem solving',
            'gradient_type': 'reasoning_gradient',
            'role_description': 'Specializes in enhancing mathematical reasoning processes, ensuring clear step-by-step logic and proper problem decomposition'
        },
        'calculation_optimizer': {
            'focus': 'Numerical computation accuracy and methodology',
            'expertise': 'Optimize calculation methods, numerical precision, and computational approaches',
            'gradient_type': 'calculation_gradient',
            'role_description': 'Focuses on improving calculation accuracy, suggesting better computational methods and ensuring numerical correctness'
        },
        'problem_interpreter': {
            'focus': 'Word problem comprehension and key information extraction',
            'expertise': 'Enhance problem understanding, identify key variables, constraints, and mathematical relationships',
            'gradient_type': 'interpretation_gradient',
            'role_description': 'Specializes in interpreting math word problems, extracting relevant information, and identifying mathematical relationships'
        },
        'solution_formatter': {
            'focus': 'Mathematical solution presentation and answer format',
            'expertise': 'Design clear solution formats, ensure proper mathematical notation, and structure final answers',
            'gradient_type': 'format_gradient',
            'role_description': 'Optimizes mathematical solution presentation, ensures clear formatting and proper answer notation (e.g., #### format)'
        },
        'generic': {
            'focus': 'General mathematical prompt improvement',
            'expertise': 'Provide general mathematical reasoning improvements without specialization',
            'gradient_type': 'generic_gradient',
            'role_description': 'General mathematical agent without specialization, explores broad mathematical reasoning improvements'
        }
    }

    # ä»»åŠ¡ç±»å‹åˆ°æ™ºèƒ½ä½“ç±»å‹çš„æ˜ å°„
    TASK_AGENT_MAPPING = {
        'liar': BASE_AGENT_TYPES,
        'ethos': BASE_AGENT_TYPES,
        'jailbreak': BASE_AGENT_TYPES,
        'gsm8k': MATH_AGENT_TYPES,
        'aqua': MATH_AGENT_TYPES,
        'svamp': MATH_AGENT_TYPES,
        'classification': BASE_AGENT_TYPES,
        'default': BASE_AGENT_TYPES
    }

    def __init__(self, agent_type, task_name='default'):
        """
        åˆå§‹åŒ–ä¸“ä¸šåŒ–æ™ºèƒ½ä½“

        Args:
            agent_type: æ™ºèƒ½ä½“ç±»å‹åç§°
            task_name: ä»»åŠ¡åç§°ï¼Œç”¨äºé€‰æ‹©åˆé€‚çš„æ™ºèƒ½ä½“é…ç½®
        """
        self.task_name = task_name
        self.agent_type = agent_type

        # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æ™ºèƒ½ä½“é…ç½®
        agent_types = self.TASK_AGENT_MAPPING.get(task_name, self.BASE_AGENT_TYPES)

        if agent_type in agent_types:
            self.config = agent_types[agent_type]
        else:
            # fallbackï¼šå¦‚æœä¼ å…¥äº†æœªçŸ¥ç±»å‹ï¼Œå°±é»˜è®¤ç”¨ generic
            print(f"Warning: Unknown agent type '{agent_type}' for task '{task_name}', using generic")
            self.config = agent_types.get('generic', self.BASE_AGENT_TYPES['generic'])

        # æ™ºèƒ½ä½“çŠ¶æ€
        self.current_prompt = None
        self.gradient_history = []
        self.performance_memory = []
        self.specialization_confidence = 1.0

        print(f"Initialized {agent_type} for {task_name}: {self.config['role_description']}")

    @classmethod
    def get_available_agent_types(cls, task_name='default'):
        """è·å–æŒ‡å®šä»»åŠ¡å¯ç”¨çš„æ™ºèƒ½ä½“ç±»å‹"""
        agent_types = cls.TASK_AGENT_MAPPING.get(task_name, cls.BASE_AGENT_TYPES)
        return list(agent_types.keys())

    async def generate_specialized_gradient_async(self, prompt, error_examples, task):
        gradient_prompt = self._construct_gradient_prompt(prompt, error_examples, task)

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒLLMè°ƒç”¨ä»¥é¿å…é˜»å¡
        loop = asyncio.get_event_loop()

        def sync_llm_call():
            return call_openai(
                prompt=gradient_prompt,
                system_prompt="You are a professional prompt word optimization expert."
            )

        # å¼‚æ­¥æ‰§è¡ŒLLMè°ƒç”¨
        with ThreadPoolExecutor(max_workers=EXPERIMENT_CONFIG['max_concurrent_agents']) as executor:
            response = await loop.run_in_executor(executor, sync_llm_call)

        # è§£ææ¢¯åº¦å“åº”
        gradient = self._parse_gradient_response(response)

        # æ›´æ–°å†å²è®°å½•
        self.gradient_history.append({
            'prompt': prompt,
            'gradient': gradient,
            'context': error_examples,
            'timestamp': time.time(),
            'agent_type': self.agent_type
        })

        return gradient

    def generate_specialized_gradient(self, prompt, error_examples, task):
        gradient_prompt = self._construct_gradient_prompt(
            prompt, error_examples, task
        )

        # è°ƒç”¨LLMç”Ÿæˆæ¢¯åº¦
        response = call_openai(
            prompt=gradient_prompt,
            system_prompt="You are a professional prompt word optimization expert."
        )

        # è§£ææ¢¯åº¦å“åº”
        gradient = self._parse_gradient_response(response)

        # æ›´æ–°å†å²è®°å½•
        self.gradient_history.append({
            'prompt': prompt,
            'gradient': gradient,
            'context': error_examples,
            'timestamp': time.time()
        })

        return gradient

    def _construct_gradient_prompt(self, prompt, error_examples, task, num_feedbacks=4):
        """
        æ„å»ºé’ˆå¯¹ä»»åŠ¡ä¼˜åŒ–çš„æ¢¯åº¦æç¤º
        """
        # æ ¼å¼åŒ–é”™è¯¯æ ·æœ¬
        error_text = self._format_error_examples(error_examples)

        # æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´æç¤º
        if self.task_name == 'gsm8k' or 'math' in self.task_name:
            task_context = "mathematical word problem solving"
        else:
            task_context = "text classification"

        base_prompt = f"""
        I'm trying to write a zero-shot {task_context} prompt.
        My current prompt is: "{prompt}"
        But this prompt gets the following examples wrong:
        {error_text}

        As an expert specialized in {self.config['expertise']}, please give {num_feedbacks} reasons 
        why the prompt could have gotten these examples wrong from a {self.config['focus']} perspective.
        Focus specifically on {task_context} requirements.

        Wrap each reason with <START> and <END>
        """
        return base_prompt

    def _format_error_examples(self, error_examples):
        """æ ¼å¼åŒ–é”™è¯¯æ ·æœ¬"""
        texts = []
        if not error_examples:
            return "æš‚æ— é”™è¯¯æ ·æœ¬"

        for example in error_examples:  # åªæ˜¾ç¤ºå‰3ä¸ªåç»­å¯ä»¥æ›´æ”¹
            text = example.get('text', '')
            texts.append(text)
        texts = np.random.choice(texts, EXPERIMENT_CONFIG['error_group_size'], replace=False) if len(texts) > \
                                                                                                 EXPERIMENT_CONFIG[
                                                                                                     'error_group_size'] else texts
        return "\n".join(texts)

    def _parse_gradient_response(self, response):
        """è§£ææ¢¯åº¦å“åº” - ä½¿ç”¨å·¥å…·æ¨¡å—"""
        return MAPGDUtils.parse_gradient_response(response)


class GradientCoordinator:
    """
    Multi-Agent Gradient Coordinator for MAPGD

    Implements semantic gradient coordination through:
    1. Semantic vectorization using sentence embeddings
    2. Conflict detection via cosine similarity analysis
    3. Gradient clustering using K-means on semantic space
    4. Intelligent fusion of conflicting gradients via LLM

    Core innovation: Transforms textual gradients into semantic space
    for mathematical-like operations and conflict resolution
    """

    def __init__(self, config):
        self.fusion_method = config.get('fusion_method', 'semantic_clustering')
        self.conflict_threshold = config.get('conflict_threshold', 0.3)
        self.max_clusters = config.get('max_clusters', 5)

        # ä½¿ç”¨å…±äº«çš„è¯­ä¹‰æ¨¡å‹
        self.semantic_model = MAPGDUtils.get_semantic_model()
        # âœ… æ·»åŠ  HCGC
        self.hcgc = HypersphereConstrainedGradientClustering(config)
        self.enable_hcgc = config.get('enable_hcgc', True)
        # ADDED: åœ¨åè°ƒå™¨ä¸­æ·»åŠ å¯¹ CAAW çš„é…ç½®å¼•ç”¨
        self.enable_caaw = config.get('enable_caaw', True)

    def coordinate_gradients(self, agent_gradients, caaw_instance=None, agent_history=None, current_iteration=0):
        """
        Main gradient coordination pipeline. Now with optional CAAW integration.
        """
        print(f"Coordinating gradients from {len(agent_gradients)} agents...")

        # å§‹ç»ˆä½¿ç”¨ HCGC è¿›è¡Œèšç±»
        print("[HCGC] Using hypersphere-constrained clustering...")

        # 1. åµŒå…¥å’Œèšç±» (ä» HCGC é€»è¾‘ä¸­æå–)
        gradient_vectors, metadata = self.hcgc._embed_to_hypersphere(agent_gradients)
        if len(gradient_vectors) == 0:
            return []

        conflicts = self.hcgc._detect_conflicts(gradient_vectors, metadata)
        n_clusters = min(len(gradient_vectors), self.max_clusters)
        cluster_labels, centroids = self.hcgc._cluster_gradients(gradient_vectors, n_clusters)
        cluster_labels = self.hcgc._apply_angular_margin(gradient_vectors, cluster_labels, centroids)
        clusters = self.hcgc._organize_clusters(metadata, cluster_labels, centroids)
        print(f"[HCGC] Formed {len(clusters)} coherent gradient clusters.")

        # 2. æ£€æŸ¥æ˜¯å¦å¯ç”¨ CAAW å¹¶è¿›è¡ŒåŠ æƒèåˆ
        if self.enable_caaw and caaw_instance and agent_history is not None:
            print("[CAAW] Applying Channel-Adaptive Agent Weighting...")
            fused_gradients = self._fuse_clusters_with_caaw(
                clusters, conflicts, agent_gradients, caaw_instance, agent_history, current_iteration
            )
        else:
            # å›é€€åˆ°æ ‡å‡†çš„ï¼ˆéåŠ æƒï¼‰èåˆ
            print("[Fusion] Using uniform (non-weighted) fusion.")
            fused_gradients = self.hcgc._fuse_clusters(clusters, conflicts)

        print(f"Fusion complete: generated {len(fused_gradients)} coordinated gradients.")
        return fused_gradients

    # ADDED: æ–°å¢ä¸€ä¸ªä¸“é—¨ç”¨äº CAAW åŠ æƒèåˆçš„æ–¹æ³•
    def _fuse_clusters_with_caaw(self, clusters, conflicts, original_agent_gradients, caaw, history, iteration):
        fused_gradients = []

        # ä¸ºæœ¬æ¬¡è¿­ä»£åˆ›å»ºä¸€ä¸ª agent -> gradients çš„æ˜ å°„ï¼Œä»¥ä¾¿åœ¨åŠ æƒèåˆæ—¶æŸ¥æ‰¾
        agent_to_gradient_map = {}
        for agent_id, grads in original_agent_gradients.items():
            agent_to_gradient_map[agent_id] = [MAPGDUtils.extract_gradient_text(g) for g in grads]

        for cluster in clusters:
            if len(cluster['gradients']) == 1:
                fused_gradients.append(cluster['gradients'][0])
                continue

            # 1. è·å–å‚ä¸æ­¤é›†ç¾¤çš„æ‰€æœ‰ agent
            agent_ids_in_cluster = list(cluster['agents'])

            # 2. è®¡ç®—è¿™äº› agent çš„æƒé‡
            agent_weights = caaw.compute_agent_weights(agent_ids_in_cluster, history, iteration)

            # 3. ä½¿ç”¨ CAAW çš„åŠ æƒèåˆæ–¹æ³•
            fused_gradient = caaw.weighted_fusion(
                gradients=cluster['gradients'],
                agent_weights=agent_weights,
                agent_to_gradient_map=agent_to_gradient_map
            )
            fused_gradients.append(fused_gradient)

        return fused_gradients

    def _vectorize_gradients(self, agent_gradients):
        """å°†æ–‡æœ¬æ¢¯åº¦è½¬æ¢ä¸ºè¯­ä¹‰å‘é‡ - ä½¿ç”¨å…±äº«å·¥å…·"""
        vectors = {}

        # æ”¶é›†æ‰€æœ‰æ¢¯åº¦æ–‡æœ¬ç”¨äºæ‰¹é‡å¤„ç†
        all_texts = []
        text_to_agent = {}

        for agent_id, gradients in agent_gradients.items():
            for i, gradient in enumerate(gradients):
                gradient_text = MAPGDUtils.extract_gradient_text(gradient)
                all_texts.append(gradient_text)
                text_to_agent[len(all_texts) - 1] = (agent_id, i)

        # æ‰¹é‡ç¼–ç æ‰€æœ‰æ–‡æœ¬ï¼ˆä½¿ç”¨å…±äº«å·¥å…·ï¼‰
        print(f"Batch encoding {len(all_texts)} gradient texts...")
        all_vectors = MAPGDUtils.encode_texts(all_texts)

        # å°†å‘é‡é‡æ–°åˆ†é…ç»™å¯¹åº”çš„æ™ºèƒ½ä½“
        for agent_id in agent_gradients.keys():
            vectors[agent_id] = []

        for idx, vector in enumerate(all_vectors):
            agent_id, _ = text_to_agent[idx]
            vectors[agent_id].append(vector)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        for agent_id in vectors:
            vectors[agent_id] = np.array(vectors[agent_id])

        return vectors

    def _detect_gradient_conflicts(self, gradient_vectors):
        """æ£€æµ‹æ¢¯åº¦æ–¹å‘å†²çª - ä½¿ç”¨å…±äº«å·¥å…·"""
        conflicts = []

        agent_ids = list(gradient_vectors.keys())

        for i, agent_a in enumerate(agent_ids):
            for agent_b in agent_ids[i + 1:]:

                vectors_a = gradient_vectors[agent_a]
                vectors_b = gradient_vectors[agent_b]
                sim_mat = vectors_a @ vectors_b.T / (
                            np.linalg.norm(vectors_a, axis=1)[:, None] * np.linalg.norm(vectors_b, axis=1)[None, :])
                idx = np.where(sim_mat < -self.conflict_threshold)
                for i_a, i_b in zip(*idx):
                    conflicts.append({'agents': (agent_a, agent_b), 'indices': (i_a, i_b), 'sim': sim_mat[i_a, i_b]})
        return conflicts

    def _cluster_gradients(self, agent_gradients, gradient_vectors):
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦èšç±»æ¢¯åº¦"""
        # å°†æ‰€æœ‰æ¢¯åº¦å‘é‡åˆå¹¶
        all_vectors = []
        gradient_metadata = []

        for agent_id, gradients in agent_gradients.items():
            vectors = gradient_vectors[agent_id]
            for i, (gradient, vector) in enumerate(zip(gradients, vectors)):
                all_vectors.append(vector)
                gradient_metadata.append({
                    'agent_id': agent_id,
                    'gradient_idx': i,
                    'gradient': gradient
                })

        # K-meansèšç±»
        n_clusters = min(len(all_vectors), 5)  # æœ€å¤š5ä¸ªé›†ç¾¤
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(all_vectors)

        # ç»„ç»‡èšç±»ç»“æœ
        clusters = []
        for cluster_id in range(n_clusters):
            cluster_gradients = []
            cluster_agents = set()

            for i, label in enumerate(cluster_labels):
                if label == cluster_id:
                    metadata = gradient_metadata[i]
                    cluster_gradients.append(metadata['gradient'])
                    cluster_agents.add(metadata['agent_id'])

            clusters.append({
                'id': cluster_id,
                'gradients': cluster_gradients,
                'agents': list(cluster_agents),
                'centroid': kmeans.cluster_centers_[cluster_id]
            })

        return clusters

    def _fuse_gradient_clusters(self, clusters, conflicts):
        """èåˆæ¢¯åº¦é›†ç¾¤"""
        fused_gradients = []

        for cluster in clusters:
            if len(cluster['gradients']) == 1:
                # å•ä¸€æ¢¯åº¦ç›´æ¥ä½¿ç”¨
                fused_gradients.append(cluster['gradients'][0])
            else:
                # å¤šæ¢¯åº¦èåˆ
                fusion_prompt = self._create_fusion_prompt(
                    cluster['gradients'], conflicts
                )

                # ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½èåˆ
                response = call_openai(
                    prompt=fusion_prompt,
                    system_prompt="You are a professional prompt word optimization expert who is good at integrating multiple improvement suggestions."
                )

                fused_gradient = self._parse_fusion_response(response)
                fused_gradients.append(fused_gradient)

        return fused_gradients

    def _create_fusion_prompt(self, gradients, conflicts, num_gradients=4):
        """åˆ›å»ºæ¢¯åº¦èåˆæç¤º"""
        gradient_texts = []
        for i, gradient in enumerate(gradients):
            gradient_text = MAPGDUtils.extract_gradient_text(gradient)
            gradient_texts.append(f"å»ºè®®{i + 1}: {gradient_text}")

        conflict_info = ""
        if conflicts:
            conflict_info = f"""

            The following potential conflicts have been detected and need to be resolved:
            {self._format_conflicts(conflicts)}
            """
        gradients = "\n".join(gradient_texts)
        fusion_prompt = f"""

        I need to combine the following multiple prompt improvement suggestions into {num_gradients} unified 
        and coherent prompt improvements:

        {gradients}
        {conflict_info}

        Wrap each unified and coherent prompt improvement with <START> and <END>
        """

        return fusion_prompt

    def _parse_fusion_response(self, response):
        """è§£æèåˆå“åº” - ä½¿ç”¨å·¥å…·æ¨¡å—"""
        return MAPGDUtils.parse_fusion_response(response)

    def _format_conflicts(self, conflicts):
        """æ ¼å¼åŒ–å†²çªä¿¡æ¯ - ä½¿ç”¨å·¥å…·æ¨¡å—"""
        return MAPGDUtils.format_conflicts(conflicts)


class BanditBasedSelector:
    """åŸºäºå¤šè‡‚èµŒåšæœºçš„é«˜æ•ˆå€™é€‰é€‰æ‹©å™¨"""

    def __init__(self, config):
        self.config = config  # ä¿å­˜é…ç½®ä»¥ä¾›åç»­ä½¿ç”¨
        self.selection_strategy = config.get('selection_strategy', 'ucb')  # 'ucb', 'thompson', 'epsilon_greedy'
        self.evaluation_budget = config.get('evaluation_budget', 50)
        self.min_evaluations = config.get('min_evaluations_per_candidate', 3)
        self.async_evaluation = config.get('async_evaluation', True)  # å¯ç”¨å¼‚æ­¥è¯„ä¼°
        self.max_concurrent_evaluations = config.get('max_concurrent_evaluations', 4)

    # def evaluate_prompts(self, candidate_prompts, training_data, task, predictor):
    #     if not candidate_prompts:
    #         return []
    #     print(f"Evaluating {len(candidate_prompts)} candidate prompts...")
    #     scores = []
    #     eval_sample_size = min(50, len(training_data))  # é€‚ä¸­çš„æ ·æœ¬å¤§å°
    #     eval_data = random.sample(training_data, eval_sample_size)

    #     print(f"  Using {eval_sample_size} samples for evaluation")

    #     for i, candidate in enumerate(candidate_prompts):
    #         try:
    #             print(f"  Evaluating candidate {i+1}/{len(candidate_prompts)}...")
    #             score, _, _, _ = task.evaluate_prompt(
    #                 candidate, eval_data, predictor, n=eval_sample_size
    #             )
    #             scores.append(score)
    #             print(f"    Score: {score:.3f}")
    #         except Exception as e:
    #             print(f"    Failed: {e}")
    #             scores.append(0.0)
    #     print(f"Evaluation completed. Scores: {[f'{s:.3f}' for s in scores]}")
    #     return scores
    def evaluate_prompts(self, candidate_prompts, training_data, task, predictor):
        if not candidate_prompts:
            return []
        print(f"Evaluating {len(candidate_prompts)} candidate prompts...")
        scores = [0.0] * len(candidate_prompts)
        eval_sample_size = min(150, len(training_data))
        eval_data = random.sample(training_data, eval_sample_size)

        print(f"  Using {eval_sample_size} samples for evaluation")

        # å¹¶å‘æ•°ä» liar_config.EXPERIMENT_CONFIG è¯»å–
        max_workers = EXPERIMENT_CONFIG.get('max_concurrent_evaluators', 4)
        print(f"  Running up to {max_workers} concurrent evaluations")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(
                    self._eval_single, candidate, eval_data, task, predictor, eval_sample_size
                ): idx
                for idx, candidate in enumerate(candidate_prompts)
            }
            for future in as_completed(futures):
                idx = futures[future]
                try:
                    score = future.result()
                    print(f"    Candidate {idx + 1}/{len(candidate_prompts)} Score: {score:.3f}")
                except Exception as e:
                    print(f"    Candidate {idx + 1} Failed: {e}")
                    score = 0.0
                scores[idx] = score

        print(f"Evaluation completed. Scores: {[f'{s:.3f}' for s in scores]}")
        return scores

    def _eval_single(self, candidate, eval_data, task, predictor, n):
        """Helper: evaluate one prompt returning only score."""
        score, _, _, _ = task.evaluate_prompt(candidate, eval_data, predictor, n=n)
        return score

    def select_top_candidates(self, candidate_prompts, task, training_data):
        """ä½¿ç”¨Banditç®—æ³•é«˜æ•ˆé€‰æ‹©æœ€ä½³å€™é€‰"""
        if not candidate_prompts:
            return []

        if self.selection_strategy == 'ucb':
            return self._ucb_selection(candidate_prompts, task, training_data)
        elif self.selection_strategy == 'thompson':
            return self._thompson_sampling(candidate_prompts, task, training_data)
        else:
            return self._epsilon_greedy_selection(candidate_prompts, task, training_data)

    def _ucb_selection(self, candidates, task, training_data):
        print(f"UCB Selection: evaluating {len(candidates)} candidates")

        # UCBå‚æ•°è®¾ç½® (å‚è€ƒevaluators.py)
        c = self.config.get('c', 1.0)  # UCBç½®ä¿¡å‚æ•°
        num_rounds = min(20, self.evaluation_budget // len(candidates))  # å­¦æœ¯å®éªŒè½®æ•°
        samples_per_eval = min(10, len(training_data) // 4)  # æ¯è½®è¯„ä¼°æ ·æœ¬æ•°

        print(f"  UCB rounds: {num_rounds}, samples per eval: {samples_per_eval}")

        # åˆå§‹åŒ–UCBç»Ÿè®¡ (å‚è€ƒUCBBandits class)
        counts = np.zeros(len(candidates))
        scores = np.zeros(len(candidates))

        # UCBä¸»å¾ªç¯ (ç®€åŒ–ç‰ˆbandit algorithm)
        for round_idx in range(num_rounds):
            print(f"  Round {round_idx + 1}/{num_rounds}")

            # è®¡ç®—UCBå€¼é€‰æ‹©å€™é€‰ (åŸºäºevaluators.pyçš„chooseæ–¹æ³•)
            if np.sum(counts) == 0:
                # åˆå§‹è½®ï¼šéšæœºé€‰æ‹©
                selected_indices = list(range(len(candidates)))
            else:
                # UCBé€‰æ‹© (å‚è€ƒUCBBandits.choose)
                avg_scores = np.divide(scores, counts + 1e-3,
                                       out=np.zeros_like(scores), where=counts > 0)
                confidence = c * np.sqrt(np.log(round_idx + 1) / (counts + 1e-3))
                ucb_values = avg_scores + confidence

                # é€‰æ‹©top-kä¸ªå€™é€‰è¿›è¡Œè¯„ä¼°
                k = min(len(candidates), 3)  # é™åˆ¶å¹¶å‘è¯„ä¼°æ•°
                selected_indices = np.argsort(ucb_values)[::-1][:k]

            # è¯„ä¼°é€‰ä¸­çš„å€™é€‰ (ç®€åŒ–è¯„ä¼°é€»è¾‘)
            eval_data = random.sample(training_data, samples_per_eval)

            for idx in selected_indices:
                try:
                    # ç›´æ¥ä½¿ç”¨task.evaluate_prompt (é¿å…å¤æ‚çš„_evaluate_candidate)
                    score, _, _, _ = task.evaluate_prompt(
                        candidates[idx], eval_data, None, n=samples_per_eval
                    )

                    # æ›´æ–°UCBç»Ÿè®¡ (å‚è€ƒUCBBandits.update)
                    counts[idx] += samples_per_eval
                    scores[idx] += score * samples_per_eval

                    avg_score = scores[idx] / counts[idx] if counts[idx] > 0 else 0
                    print(f"    Candidate {idx}: score={score:.3f}, avg={avg_score:.3f}")

                except Exception as e:
                    print(f"    Candidate {idx}: evaluation failed ({e})")
                    counts[idx] += 1  # ä»ç„¶è®¡æ•°ï¼Œé¿å…é‡å¤é€‰æ‹©

        # è¿”å›æœ€ç»ˆç»“æœ (å‚è€ƒUCBBandits.get_scores)
        final_scores = np.divide(scores, counts, out=np.zeros_like(scores), where=counts > 0)

        # é€‰æ‹©top-kå€™é€‰
        beam_size = self.config.get('beam_size', 3)
        top_indices = np.argsort(final_scores)[::-1][:beam_size]
        selected_candidates = [candidates[i] for i in top_indices]

        print(f"  UCB final scores: {[f'{final_scores[i]:.3f}' for i in top_indices]}")
        print(f"  Selected {len(selected_candidates)} candidates")

        return selected_candidates

    def _thompson_sampling(self, candidates, task, training_data):
        """Thompsoné‡‡æ ·ç­–ç•¥ """
        print(f"Thompson Sampling: evaluating {len(candidates)} candidates")

        # åŸºäºBetaåˆ†å¸ƒçš„Thompsoné‡‡æ ·
        candidate_scores = []
        eval_sample_size = min(10, len(training_data))
        eval_data = random.sample(training_data, eval_sample_size)

        for i, candidate in enumerate(candidates):
            try:
                # è¯„ä¼°å€™é€‰
                score, _, _, _ = task.evaluate_prompt(
                    candidate, eval_data, None, n=eval_sample_size
                )

                # Thompsoné‡‡æ ·ï¼šä»Betaåˆ†å¸ƒé‡‡æ ·
                alpha = 1 + score * 10  # æˆåŠŸæ¬¡æ•°
                beta = 1 + (1 - score) * 10  # å¤±è´¥æ¬¡æ•°
                sampled_score = np.random.beta(alpha, beta)

                candidate_scores.append((sampled_score, candidate))
                print(f"  Candidate {i + 1}: score={score:.3f}, sampled={sampled_score:.3f}")

            except Exception as e:
                print(f"  Candidate {i + 1}: failed ({e})")
                candidate_scores.append((0.0, candidate))

        # æŒ‰é‡‡æ ·åˆ†æ•°æ’åºï¼Œè¿”å›top-k
        candidate_scores.sort(reverse=True, key=lambda x: x[0])
        beam_size = self.config.get('beam_size', 3)
        return [candidate for _, candidate in candidate_scores[:beam_size]]

    def _epsilon_greedy_selection(self, candidates, task, training_data):
        """Epsilonè´ªå¿ƒç­–ç•¥"""
        epsilon = 0.1  # æ¢ç´¢ç‡
        print(f"Epsilon-Greedy (Îµ={epsilon}): evaluating {len(candidates)} candidates")

        beam_size = self.config.get('beam_size', 3)

        if random.random() < epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©
            print("  Using exploration (random selection)")
            return random.sample(candidates, min(beam_size, len(candidates)))
        else:
            # åˆ©ç”¨ï¼šè´ªå¿ƒé€‰æ‹©æœ€ä½³
            print("  Using exploitation (greedy selection)")
            candidate_scores = []
            eval_sample_size = min(8, len(training_data))
            eval_data = random.sample(training_data, eval_sample_size)

            for i, candidate in enumerate(candidates):
                try:
                    score, _, _, _ = task.evaluate_prompt(
                        candidate, eval_data, None, n=eval_sample_size
                    )
                    candidate_scores.append((score, candidate))
                    print(f"  Candidate {i + 1}: score={score:.3f}")
                except Exception as e:
                    print(f"  Candidate {i + 1}: failed ({e})")
                    candidate_scores.append((0.0, candidate))

            # è¿”å›top-kå€™é€‰
            candidate_scores.sort(reverse=True, key=lambda x: x[0])
            return [candidate for _, candidate in candidate_scores[:beam_size]]


class AgentManager:
    """
    Multi-Agent Manager for MAPGD Framework

    Orchestrates collaboration between 4 specialized agents:
    - instruction_specialist: Optimizes task instructions
    - example_curator: Manages few-shot examples
    - format_designer: Structures output formats
    - style_optimizer: Refines language style

    Implements parallel gradient generation and agent synchronization
    """

    def __init__(self, config):
        self.config = config
        self.num_agents = config.get('num_agents', 4)
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ­£ç¡®è·å–ä»»åŠ¡åç§°
        self.task_name = config.get('task_name', 'default')
        print(f"ğŸ” AgentManager initialized for task: {self.task_name}")  # æ·»åŠ è°ƒè¯•ä¿¡æ¯

        self.synchronization_strategy = config.get('sync_strategy', 'best_prompt_sharing')
        self.agents = []
        self.async_mode = config.get('async_mode', True)
        self.max_concurrent_agents = config.get('max_concurrent_agents', 4)

    def initialize_agents(self, initial_prompt):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“ (æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€é€‰æ‹©ä¸“ä¸šåŒ–è§’è‰²)"""

        # æ ¹æ®ä»»åŠ¡è·å–å¯ç”¨çš„æ™ºèƒ½ä½“ç±»å‹
        available_agent_types = SpecializedPromptAgent.get_available_agent_types(self.task_name)

        random_roles = self.config.get("random_agent_roles", False)
        mode = "random" if random_roles else "specialized"

        print(f"Initializing {self.num_agents} {mode} agents for task '{self.task_name}'...")
        print(f"Available agent types: {available_agent_types}")

        for i in range(self.num_agents):
            if random_roles:
                # éšæœºé€‰æ‹©ä¸€ä¸ªè§’è‰²
                agent_type = random.choice(available_agent_types)
            else:
                # é»˜è®¤é€»è¾‘ï¼šå¾ªç¯åˆ†é…è§’è‰²
                agent_type = available_agent_types[i % len(available_agent_types)]

            # åˆ›å»ºæ™ºèƒ½ä½“æ—¶ä¼ å…¥ä»»åŠ¡åç§°
            agent = SpecializedPromptAgent(agent_type, task_name=self.task_name)
            agent.current_prompt = initial_prompt
            self.agents.append(agent)

            print(f"  Agent {i + 1}: {agent_type} - {agent.config['role_description']}")

        print(
            f"Agent initialization complete for {self.task_name}. Ready for {'async' if self.async_mode else 'sync'} optimization.")

    async def generate_gradients_async(self, training_data, task, predictor):
        """
        Asynchronous Parallel Gradient Generation - Core Performance Optimization
        All agents perform parallel analysis simultaneously, avoiding serial waiting:
        1. Parallel data sampling and error analysis
        2. Parallel gradient generation
        3. Unified result collection
        """

        print(f"Starting gradient generation from {len(self.agents)} agents...")
        start_time = time.time()

        async def generate_single_agent_gradient(agent_idx, agent):
            """å•ä¸ªæ™ºèƒ½ä½“çš„å¼‚æ­¥æ¢¯åº¦ç”Ÿæˆä»»åŠ¡"""
            try:
                print(f"  Agent {agent_idx + 1} ({agent.agent_type}): starting analysis...")

                # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“é‡‡æ ·ä¸åŒçš„æ•°æ®å­é›†ï¼Œå¢åŠ æ¢ç´¢å¤šæ ·æ€§
                sample_size = min(20, len(training_data))
                start_idx = (agent_idx * 10) % len(training_data)
                end_idx = min(start_idx + sample_size, len(training_data))

                if end_idx - start_idx < sample_size:
                    sample_data = random.sample(training_data, min(sample_size, len(training_data)))
                else:
                    sample_data = training_data[start_idx:end_idx]

                # è¯„ä¼°å½“å‰æç¤ºæ€§èƒ½ï¼Œè·å–é”™è¯¯æ ·æœ¬
                score, texts, labels, preds = task.evaluate_prompt(
                    agent.current_prompt, sample_data, predictor, n=len(sample_data)
                )

                # æ„é€ é”™è¯¯æ ·æœ¬
                error_examples = []
                for text, label, pred in zip(texts, labels, preds):
                    if label != pred:
                        error_examples.append({
                            'text': text,
                            'true_label': label,
                            'predicted_label': pred
                        })
                # å¼‚æ­¥ç”Ÿæˆæ¢¯åº¦
                gradients = await agent.generate_specialized_gradient_async(
                    agent.current_prompt, error_examples, task
                )

                print(f"  Agent {agent_idx + 1} completed: {len(gradients)} gradients")
                return f'agent_{agent_idx}', gradients

            except Exception as e:
                print(f"  Agent {agent_idx + 1} failed: {e}")
                return f'agent_{agent_idx}', [f"Improve the prompt for better {agent.config['focus']}"]

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ™ºèƒ½ä½“çš„æ¢¯åº¦ç”Ÿæˆ
        tasks = [
            generate_single_agent_gradient(i, agent)
            for i, agent in enumerate(self.agents)
        ]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # æ•´ç†ç»“æœ
        agent_gradients = {}
        successful_agents = 0

        for result in results:
            if isinstance(result, tuple):
                agent_id, gradients = result
                agent_gradients[agent_id] = gradients
                successful_agents += 1
            else:
                print(f"  Task failed with exception: {result}")

        total_time = time.time() - start_time
        print(f"gradient generation completed:")
        print(f"  - {successful_agents}/{len(self.agents)} agents successful")
        print(f"  - Total time: {total_time:.2f}s")
        print(f"  - Average time per agent: {total_time / len(self.agents):.2f}s")

        return agent_gradients

    def generate_gradients(self, training_data, task, predictor):
        """
        æ™ºèƒ½ä½“æ¢¯åº¦ç”Ÿæˆå…¥å£ - æ”¯æŒåŒæ­¥/å¼‚æ­¥æ¨¡å¼åˆ‡æ¢

        æ ¹æ®é…ç½®é€‰æ‹©æ‰§è¡Œæ¨¡å¼ï¼š
        - async_mode=True: å¹¶è¡Œå¼‚æ­¥æ‰§è¡Œæ‰€æœ‰æ™ºèƒ½ä½“
        - async_mode=False: ä¸²è¡ŒåŒæ­¥æ‰§è¡Œï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
        """

        if self.async_mode:
            # å¼‚æ­¥æ¨¡å¼ï¼šå¹¶è¡Œæ‰§è¡Œ
            print("Using ASYNC mode for gradient generation...")
            return asyncio.run(self.generate_gradients_async(training_data, task, predictor))
        else:
            # åŒæ­¥æ¨¡å¼ï¼šä¸²è¡Œæ‰§è¡Œï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            print("Using SYNC mode for gradient generation...")
            return self._generate_gradients_sync(training_data, task, predictor)

    def _generate_gradients_sync(self, training_data, task, predictor):
        """
        åŒæ­¥ç‰ˆæœ¬æ¢¯åº¦ç”Ÿæˆ - ä¿æŒå‘åå…¼å®¹æ€§
        """
        agent_gradients = {}

        print(f"Generating specialized gradients from {len(self.agents)} agents...")

        for i, agent in enumerate(self.agents):
            print(f"  Agent {i + 1} ({agent.agent_type}): analyzing prompt performance...")

            # Sample training data for agent evaluation
            sample_size = min(20, len(training_data))
            sample_data = random.sample(training_data, sample_size)

            # ä½¿ç”¨å½“å‰æç¤ºè¯„ä¼°æ€§èƒ½ï¼Œè·å–é”™è¯¯æ ·æœ¬
            try:
                score, texts, labels, preds = task.evaluate_prompt(
                    agent.current_prompt, sample_data, predictor, n=len(sample_data)
                )

                # æ„é€ é”™è¯¯æ ·æœ¬
                error_examples = []
                for text, label, pred in zip(texts, labels, preds):
                    if label != pred:
                        error_examples.append({
                            'text': text,
                            'true_label': label,
                            'predicted_label': pred
                        })

                # å¦‚æœæ²¡æœ‰é”™è¯¯æ ·æœ¬ï¼Œéšæœºé€‰æ‹©ä¸€äº›æ ·æœ¬
                if not error_examples:
                    error_examples = [
                        {
                            'text': ex.get('text', ''),
                            'true_label': ex.get('label', ''),
                            'predicted_label': ex.get('label', '')
                        }
                        for ex in sample_data[:3]
                    ]

                # ç”Ÿæˆæ¢¯åº¦
                gradients = agent.generate_specialized_gradient(
                    agent.current_prompt, error_examples, task
                )

                agent_gradients[f'agent_{i}'] = gradients

            except Exception as e:
                print(f"Error generating gradients for agent {i}: {e}")
                agent_gradients[f'agent_{i}'] = [f"Improve the prompt for better {agent.config['focus']}"]

        return agent_gradients

    def get_current_prompts(self):
        """è·å–å½“å‰æç¤º"""
        return [agent.current_prompt for agent in self.agents]

    def update_agents(self, selected_prompts):
        """æ›´æ–°æ™ºèƒ½ä½“çŠ¶æ€"""
        for i, agent in enumerate(self.agents):
            if i < len(selected_prompts):
                agent.current_prompt = selected_prompts[i]


class CollaborativePromptExpander:
    """
    Collaborative Prompt Expansion for MAPGD

    Implements prompt space exploration through:
    1. Beam Search: Systematic expansion guided by semantic gradients
    2. Monte Carlo Paraphrasing: Stochastic variations for diversity
    3. Gradient-Prompt Fusion: Direct application of textual gradients
    4. Diversity Maximization: Ensuring exploration coverage

    Key innovation: Combines deterministic beam search with stochastic
    Monte Carlo sampling for balanced exploration-exploitation
    """

    def __init__(self, config):
        self.beam_size = config.get('beam_size', 3)
        self.mc_samples = config.get('mc_samples', 2)  # Monte Carlo samples per prompt
        self.diversity_threshold = config.get('diversity_threshold', 0.7)
        self.expansion_strategy = config.get('expansion_strategy',
                                             'beam_search')  # 'beam_search', 'monte_carlo', 'hybrid'

        # ä½¿ç”¨å…±äº«çš„è¯­ä¹‰æ¨¡å‹
        self.semantic_model = MAPGDUtils.get_semantic_model()
        if self.semantic_model:
            print(f"CollaborativePromptExpander: Using shared semantic model for diversity filtering")
        else:
            print(f"Warning: No semantic model available for diversity filtering")

    def expand(self, current_prompt, gradients):
        """
        Collaborative prompt expansion pipeline - Strategy-based approach:

        Strategy options:
        - 'beam_search': Systematic gradient application (deterministic)
        - 'monte_carlo': Stochastic paraphrasing variations (exploratory)
        - 'hybrid': Balanced combination of both approaches

        Returns: Expanded set of candidate prompts based on selected strategy
        """

        print(f"Expanding prompts with strategy: {self.expansion_strategy}")

        candidates = []

        if self.expansion_strategy == 'beam_search':
            # Strategy 1: Pure beam search with gradient application
            candidates = self._beam_search_expansion(current_prompt, gradients)
            print(f"Beam search strategy generated {len(candidates)} candidates")

        elif self.expansion_strategy == 'monte_carlo':
            # Strategy 2: Pure Monte Carlo paraphrasing
            candidates = self._monte_carlo_paraphrasing(current_prompt)
            print(f"Monte Carlo strategy generated {len(candidates)} variants")

        diverse_candidates = self._filter_for_diversity(candidates)
        print(f"Filtered to {len(diverse_candidates)} diverse candidates")

        return diverse_candidates[:self.beam_size * 2]  # Return reasonable number of candidates

    def _beam_search_expansion(self, current_prompt, gradients):
        """Systematic beam search guided by semantic gradients"""
        beam_candidates = []

        for gradient in gradients:
            # Apply gradient to generate new candidate
            expanded_prompt = self._apply_gradient(current_prompt, gradient)
            for i in range(len(expanded_prompt)):
                beam_candidates.append(expanded_prompt[i])

        return beam_candidates

    def _monte_carlo_paraphrasing(self, base_prompt):
        """Stochastic Monte Carlo paraphrasing for diversity"""
        mc_variants = []

        # Limit MC to top prompts
        for _ in range(self.mc_samples):
            variant = self._generate_stochastic_variant(base_prompt)
            mc_variants.append(variant[0])

        return mc_variants

    def _apply_gradient(self, prompt, gradient, steps_per_gradient=1):
        """Apply semantic gradient to prompt via LLM"""
        application_prompt = f"""
        I'm trying to write a zero-shot classifier.
        My current prompt is:
        "{prompt}"
        the gradient of this prompt is {gradient}
        Based on the above information, I wrote{steps_per_gradient} different improved prompts.
        The {steps_per_gradient} new prompts are wrapped with <START> and <END>:
        """

        response = call_openai(
            prompt=application_prompt,
            system_prompt="You are a professional expert in prompt word optimization, skilled in applying semantic gradients for precise improvements."
        )

        return MAPGDUtils.parse_gradient_response(response)

    def _generate_stochastic_variant(self, prompt):
        """Generate stochastic Monte Carlo variant"""
        # paraphrase_styles = [
        #     "æ›´ç®€æ´æ¸…æ™°çš„è¡¨è¾¾æ–¹å¼",
        #     "æ›´æ­£å¼ä¸“ä¸šçš„è¯­è¨€é£æ ¼",
        #     "æ›´å…·ä½“è¯¦ç»†çš„æŒ‡ä»¤æè¿°",
        #     "æ›´å‹å¥½æ˜“æ‡‚çš„è¡¨è¾¾æ–¹å¼"
        # ]

        # style = random.choice(paraphrase_styles)

        variant_prompt = f"""
        Generate a variation of the following instruction while keeping the semantic meaning.

        Input: {prompt}
        Each output prompt is wrapped with <START> and <END>.
        """

        response = call_openai(
            prompt=variant_prompt,
            system_prompt=""
        )

        return MAPGDUtils.parse_gradient_response(response)

    def _filter_for_diversity(self, candidates):
        """Filter candidates for diversity using shared semantic tools"""
        return MAPGDUtils.filter_for_diversity(
            candidates,
            diversity_threshold=self.diversity_threshold,
            max_candidates=self.beam_size * 2
        )


class ConvergenceMonitor:
    """æ”¶æ•›ç›‘æ§å™¨"""

    def __init__(self, config):
        self.convergence_threshold = config.get('convergence_threshold', 0.01)
        self.patience = config.get('patience', 3)
        self.performance_history = []
        self.no_improvement_count = 0

    def check_convergence(self, current_performance=None):
        """æ£€æŸ¥æ˜¯å¦æ”¶æ•›"""
        if current_performance is not None:
            self.performance_history.append(current_performance)

        if len(self.performance_history) < 2:
            return False

        # æ£€æŸ¥æ€§èƒ½æå‡
        recent_improvement = (
                self.performance_history[-1] - self.performance_history[-2]
        )

        if recent_improvement < self.convergence_threshold:
            self.no_improvement_count += 1
        else:
            self.no_improvement_count = 0

        return self.no_improvement_count >= self.patience
