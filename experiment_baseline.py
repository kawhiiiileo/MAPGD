"""
Baseline-Compatible MAPGDå®éªŒè„šæœ¬
ä¸¥æ ¼åŒ¹é…baselineè®ºæ–‡çš„å®éªŒè®¾ç½®å’Œå‚æ•°
"""

import os
import time
import json
import numpy as np
from core import MAPGDFramework
from mapgd_tasks import get_mapgd_task_class
from mapgd_predictors import get_mapgd_predictor
from init_prompt import TEST_INITIAL_PROMPT, JAILBREAK_INITIAL_PROMPT
from liar_config import EXPERIMENT_CONFIG as LIAR_CONFIG
from jailbreak_config import EXPERIMENT_CONFIG as JAILBREAK_CONFIG
from ethos_config import EXPERIMENT_CONFIG as ETHOS_CONFIG
from gsm8k_config import EXPERIMENT_CONFIG as GSM8K_CONFIG
from aqua_config import EXPERIMENT_CONFIG as AQUA_CONFIG
from svamp_config import EXPERIMENT_CONFIG as SVAMP_CONFIG
from hcgc_caaw import ChannelAdaptiveAgentWeighting


class BaselineCompatibleMAPGDFramework(MAPGDFramework):
    def __init__(self, config):
        super().__init__(config)
        # å¯ç”¨ CAAW
        if config.get('enable_caaw', True):
            print("âœ… Channel-Adaptive Agent Weighting (CAAW) is ENABLED.")
            self.caaw = ChannelAdaptiveAgentWeighting(config)
            self.agent_performance_history = []
        else:
            print("âŒ Channel-Adaptive Agent Weighting (CAAW) is DISABLED.")
            self.caaw = None
            self.agent_performance_history = None
    """
    Baselineå…¼å®¹çš„MAPGDæ¡†æ¶

    å®ç°baselineè®ºæ–‡ä¸­çš„æ ‡å‡†é…ç½®ï¼š
    - minibatch size = 64
    - beam size = 4
    - 6 optimization steps
    - 4 errors per group, 4 gradients per group
    - 2 monte carlo samples per candidate
    - 8 successor candidates per parent
    - bandit selection
    """

    def optimize_prompt(self, initial_prompt, task_name, data_dir, predictor=None,
                        predictor_type='binary_classification'):
        """ä¸»ä¼˜åŒ–æµç¨‹ - ä¿®å¤é¢„æµ‹å™¨åˆå§‹åŒ–é—®é¢˜"""
        start_time = time.time()

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿configä¸­åŒ…å«task_name
        if 'task_name' not in self.config:
            self.config['task_name'] = task_name

        print(f"ğŸ” Task name in config: {self.config.get('task_name')}")
        print(f"ğŸ” Predictor type: {predictor_type}")

        # åŠ è½½ä»»åŠ¡å’Œæ•°æ®
        task_class = get_mapgd_task_class(task_name)
        task = task_class(data_dir)
        print(f"ğŸ” task.data_dir = {task.data_dir}")
        train_examples = task.get_train_examples()
        test_examples = task.get_test_examples()

        print(f"ğŸ“Š Dataset: {len(train_examples)} train, {len(test_examples)} test examples")

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿é¢„æµ‹å™¨æ­£ç¡®åˆå§‹åŒ–
        if predictor is None:
            print(f"ğŸ”§ Creating predictor of type: {predictor_type}")
            predictor = get_mapgd_predictor(predictor_type, self.config)
            print(f"âœ… Predictor created: {type(predictor)}")

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿é¢„æµ‹å™¨ä¸ä¸ºNone
        if predictor is None:
            raise ValueError(f"Failed to create predictor of type: {predictor_type}")

        # åˆå§‹åŒ–æ™ºèƒ½ä½“ - ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ä»»åŠ¡åç§°
        print(f"ğŸ¤– Initializing agents for task: {task_name}")
        self.agent_manager.initialize_agents(initial_prompt)

        # å…¶ä½™ä»£ç ä¿æŒä¸å˜...
        optimization_history = []
        best_overall_prompt = initial_prompt
        best_overall_score = 0.0
        current_best_prompt = initial_prompt
        last_iteration_test_score = 0.0

        # ========== BASELINEä¸»ä¼˜åŒ–å¾ªç¯ (6æ­¥) ==========
        for iteration in range(self.max_iterations):
            print(f"\nğŸ”„ === Optimization Step {iteration + 1}/{self.max_iterations} ===")
            iteration_start = time.time()

            # 1ï¸âƒ£ BASELINE: é‡‡æ ·minibatch (|Dmini| = 64)
            minibatch_size = min(self.config.get('minibatch_size', 64), len(train_examples))
            minibatch = np.random.choice(train_examples, minibatch_size, replace=False).tolist()
            print(f"ğŸ“¦ Sampled minibatch: {minibatch_size} examples")

            # 2ï¸âƒ£ BASELINE: æ¢¯åº¦ç”Ÿæˆ (4 errors at a time, m=4 gradients per group)
            print(f"ğŸ” Predictor check before gradient generation: {type(predictor)}")
            agent_gradients = self.agent_manager.generate_gradients(
                minibatch, task, predictor
            )
            print(f"ğŸ“ˆ Generated gradients from {len(agent_gradients)} agents")

            # 3ï¸âƒ£ æ¢¯åº¦åè°ƒä¸èåˆ
            if self.config.get('disable_fusion', False):
                fused_gradients = sum(agent_gradients.values(), [])  # ç›´æ¥æ‹¼æ¥æ‰€æœ‰æ¢¯åº¦
                print(f"ğŸš« Fusion disabled, using {len(fused_gradients)} raw gradients")
            else:
                fused_gradients = self.gradient_coordinator.coordinate_gradients(
                    agent_gradients,
                    caaw_instance=self.caaw,
                    agent_history=self.agent_performance_history,
                    current_iteration=iteration
                )
                print(f"ğŸ”€ Fused to {len(fused_gradients)} coordinated gradients")

            print(f"ğŸ”€ Fused to {len(fused_gradients)} coordinated gradients")

            # 4ï¸âƒ£ expansion
            candidates = self.prompt_expander.expand(
                current_prompt = current_best_prompt,  # ä½¿ç”¨èåˆæç¤ºä½œä¸ºåŸºç¡€
                gradients = fused_gradients
            )
            print(f"ğŸŒ± Generated {len(candidates)} candidate prompts")

            # 5ï¸âƒ£ BASELINE: Bandité€‰æ‹©
            eval_sample_size = min(32, len(train_examples))  # è¯„ä¼°æ ·æœ¬æ•°
            eval_samples = np.random.choice(train_examples, eval_sample_size, replace=False).tolist()

            scores = self.candidate_selector.evaluate_prompts(
                candidates, eval_samples, task, predictor
            )
            print(f"ğŸ“Š Evaluated {len(candidates)} candidates")

            # 7ï¸âƒ£ é€‰æ‹©æœ€ä½³å€™é€‰ (beam size = 4)
            prompt_score_pairs = list(zip(scores, candidates))
            prompt_score_pairs.sort(reverse=True)

            # beam_size = self.config.get('beam_size', 4)
            # best_prompts = [prompt for _, prompt in prompt_score_pairs[:beam_size]]
            # # self.agent_manager.update_agents(best_prompts)

            # 8ï¸âƒ£ è®°å½•ç»“æœ - é€‰æ‹©è¯„åˆ†æœ€é«˜çš„æç¤º
            # å¤šæ™ºèƒ½ä½“åä½œå·²ç»é€šè¿‡æ¢¯åº¦èåˆå®Œæˆï¼Œè¿™é‡Œç›´æ¥é€‰æ‹©æœ€ä½³å€™é€‰
            current_best_prompt = prompt_score_pairs[0][1]  # è¯„åˆ†æœ€é«˜çš„æç¤º
            current_best_score = prompt_score_pairs[0][0]   # å¯¹åº”çš„è¯„åˆ†

            print(f"ğŸ“ˆ Selected best prompt with score: {current_best_score:.4f}")

            # æµ‹è¯•é›†è¯„ä¼°
            test_eval_size = min(100, len(test_examples))
            test_samples = test_examples[:test_eval_size]
            test_score, _, _, _ = task.evaluate_prompt(
                current_best_prompt, test_samples, predictor, n=test_eval_size
            )
            print(f"ğŸ“Š Best prompt test score: {test_score:.4f}")
            if self.caaw and self.agent_performance_history is not None:
                performance_gain = test_score - last_iteration_test_score
                print(f"[CAAW] Performance gain this iteration: {performance_gain:.4f}")
                # å°†è¿™ä¸ªå¢ç›Šå½’åŠŸäºæ‰€æœ‰å‚ä¸æœ¬è½®çš„ agent
                for i in range(len(self.agent_manager.agents)):
                    self.agent_performance_history.append({
                        'iteration': iteration,
                        'agent_id': f'agent_{i}',
                        'improvement': performance_gain,
                        'test_score': test_score
                    })
                # æ›´æ–°ä¸Šä¸€è½®çš„åˆ†æ•°
                last_iteration_test_score = test_score
            # æ›´æ–°å…¨å±€æœ€ä½³
            if test_score > best_overall_score:
                best_overall_prompt = current_best_prompt
                best_overall_score = test_score
            
            iteration_time = time.time() - iteration_start
            
            iteration_result = {
                'iteration': iteration + 1,
                'best_prompt': current_best_prompt,
                'train_score': current_best_score,
                'test_score': test_score,
                'num_candidates': len(candidates),
                'minibatch_size': minibatch_size,
                'eval_sample_size': eval_sample_size,
                'iteration_time': iteration_time
            }
            
            optimization_history.append(iteration_result)
            
            print(f"âœ… Step {iteration + 1} Results:")
            print(f"   ğŸ“ˆ Train Score: {current_best_score:.4f}")
            print(f"   ğŸ¯ Test Score: {test_score:.4f}")
            print(f"   â±ï¸  Time: {iteration_time:.2f}s")
            print(f"   ğŸ“ Best Prompt: {current_best_prompt}")
            
            # æ”¶æ•›æ€§æ£€æŸ¥
            if self.convergence_monitor.check_convergence(test_score):
                print(f"ğŸ¯ Converged after {iteration + 1} optimization steps")
                break
            
        total_time = time.time() - start_time
        
        # è¿”å›ç»“æœ
        final_result = {
            'framework': 'MAPGD-Baseline-Compatible',
            'best_prompt': best_overall_prompt,  # ä½¿ç”¨å¤šæ™ºèƒ½ä½“èåˆçš„æœ€ç»ˆæç¤º
            'final_test_score': best_overall_score,  # ä½¿ç”¨èåˆæç¤ºçš„æµ‹è¯•å¾—åˆ†
            'optimization_history': optimization_history,
            'total_iterations': len(optimization_history),
            'total_time': total_time,
            'config': self.config,
            'baseline_compliant': True,
            'multi_agent_fusion': True  # æ ‡è®°ä½¿ç”¨äº†å¤šæ™ºèƒ½ä½“èåˆ
        }
        
        print(f"\nğŸ MAPGD Optimization Complete!")
        print(f"ğŸ“Š Final Results:")
        print(f"   ğŸ† Best Test Score: {best_overall_score:.4f}")
        print(f"   ğŸ”¢ Total Steps: {len(optimization_history)}")
        print(f"   â±ï¸  Total Time: {total_time:.2f}s")
        print(f"   ğŸ“ Final Prompt: {best_overall_prompt}")
        
        return final_result


# 2. åœ¨ run_baseline_experiment å‡½æ•°ä¸­æ›´æ–°é…ç½®é€‰æ‹©éƒ¨åˆ†ï¼š
def run_baseline_experiment(task="jailbreak"):
    """è¿è¡Œbaselineå…¼å®¹çš„MAPGDå®éªŒ"""

    # ========== æ ¹æ®ä»»åŠ¡é€‰æ‹©é…ç½® ==========
    if task == "liar":
        config = LIAR_CONFIG.copy()
        initial_prompt = LIAR_CONFIG['initial_prompt']
        predictor_type = 'binary_classification'

    elif task == "jailbreak":
        config = JAILBREAK_CONFIG.copy()
        initial_prompt = JAILBREAK_CONFIG['initial_prompt']
        predictor_type = 'binary_classification'

    elif task == "ethos":
        config = ETHOS_CONFIG.copy()
        initial_prompt = ETHOS_CONFIG['initial_prompt']
        predictor_type = 'binary_classification'

    elif task == "gsm8k":  # æ·»åŠ GSM8kåˆ†æ”¯
        config = GSM8K_CONFIG.copy()
        initial_prompt = GSM8K_CONFIG['initial_prompt']
        predictor_type = 'math_reasoning'  # ä½¿ç”¨æ•°å­¦æ¨ç†é¢„æµ‹å™¨
    elif task == "svamp":  # æ·»åŠ GSM8kåˆ†æ”¯
        config = SVAMP_CONFIG.copy()
        initial_prompt = SVAMP_CONFIG['initial_prompt']
        predictor_type = 'math_reasoning'  # ä½¿ç”¨æ•°å­¦æ¨ç†é¢„æµ‹å™¨
    elif task == "aqua":  # â­ æ–°å¢ AQuA åˆ†æ”¯
        config = AQUA_CONFIG.copy()
        initial_prompt = AQUA_CONFIG['initial_prompt']
        predictor_type = 'aqua_reasoning'  # ä½¿ç”¨ AQuA ä¸“ç”¨é¢„æµ‹å™¨

    else:
        raise ValueError(f"Unsupported task: {task}")


    
    print("ğŸš€ Starting MAPGD Experiment with Baseline-Compatible Settings")
    print("=" * 60)
    print("ğŸ“Š Experiment Configuration (Baseline Match):")
    print(f"  â€¢ Optimization Steps: {config['max_iterations']}")
    print(f"  â€¢ Beam Size: {config['beam_size']}")
    print(f"  â€¢ Minibatch Size: {config['minibatch_size']}")
    print(f"  â€¢ Error Group Size: {config['error_group_size']}")
    print(f"  â€¢ Gradients per Group: {config['gradients_per_group']}")
    print(f"  â€¢ Monte Carlo Samples: {config['monte_carlo_samples']}")
    print(f"  â€¢ Successor Candidates: {config['successor_candidates']}")
    print(f"  â€¢ Selection Strategy: {config['selection_strategy']}")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–æ¡†æ¶
        framework = BaselineCompatibleMAPGDFramework(config)
        
        # åˆå§‹æç¤º (ä½¿ç”¨ç®€å•ä½†æœ‰æ•ˆçš„åˆå§‹æç¤º)
        #initial_prompt = TEST_INITIAL_PROMPT
        
        # è¿è¡Œä¼˜åŒ–
        print(f"\nğŸ¯ Starting optimization with initial prompt...")
        print(f"Initial prompt: {initial_prompt[:80]}...")
        predictor = get_mapgd_predictor(predictor_type, config)
        results = framework.optimize_prompt(
            initial_prompt=initial_prompt,
            task_name=config['task_name'],
            data_dir=config['data_dir'],
            predictor=predictor,
            predictor_type=predictor_type  # ä¼ å…¥é¢„æµ‹å™¨ç±»å‹
        )
        
        # ä¿å­˜ç»“æœ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        result_file = f"baseline_{task}_results_{timestamp}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            results_to_save = {
                'experiment_type': 'baseline_compatible',
                'config': config,
                'results': results,
                'timestamp': timestamp
            }
            json.dump(results_to_save, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Results saved to: {result_file}")
        return results
        
    except Exception as e:
        print(f"âœ— Baseline experiment failed: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--task", type=str, default="jailbreak",
                       choices=["liar", "jailbreak", "ethos", "gsm8k", "aqua", "svamp"])  # æ·»åŠ  "gsm8k"
    args = parser.parse_args()

    print("ğŸ”¬ MAPGD Baseline-Compatible Experiment")
    print(f"ğŸ¯ Goal: Match baseline paper configuration for fair comparison (Task={args.task})")
    print()

    results = run_baseline_experiment(task=args.task)
    if results:
        print("\nâœ… Baseline-compatible experiment completed successfully!")
        print(f"ğŸ¯ Final accuracy: {results['final_test_score']:.4f}")
    else:
        print("\nâŒ Baseline-compatible experiment failed!")